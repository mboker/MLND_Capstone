{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import network.builders as bldrs\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, get all of the data, and combine the 25 headlines into one string for each row, stripping the junk characters off of the beginnings and ends of the strings, and adding the delimiter token.  Save this to a new file as a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/Combined_News_DJIA.csv', header=0, na_values=[])\n",
    "data.fillna('', inplace=True)\n",
    "data['Combined'] = ''\n",
    "for i in range(1, 26):\n",
    "    data['Combined'] += ' ||new_headline|| ' + \\\n",
    "                        data['Top'+str(i)].str.strip('b\\\\\\'\" ')\n",
    "    data = data.drop('Top'+str(i), axis=1)\n",
    "\n",
    "data.to_csv('../data/headlines_combined.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, split the combined strings into lists of words, remove stop words, and tokenize these lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#make sure to run\n",
    "#nltk.download()\n",
    "punc_tokens = {'.': '||period||',\n",
    "               ',': '||comma||',\n",
    "               '\"': '||quotation||',\n",
    "               ';': '||semicolon||',\n",
    "               '!': '||exclamation||',\n",
    "               '?': '||question||',\n",
    "               '(': '||left_paren||',\n",
    "               ')': '||right_paren||',\n",
    "               '--': '||hyphen||',\n",
    "               '-': '||hyphen||',\n",
    "               ':': '||colon||',\n",
    "               '[': '||left_brack||',\n",
    "               ']': '||right_brack||',\n",
    "               '{': '||left_curly||',\n",
    "               '}': '||right_curly||',\n",
    "               '\\n': '||newline||'}\n",
    "\n",
    "\n",
    "def _split_remove_stopwords_tokenize_punc(combined_string):\n",
    "    words = word_tokenize(combined_string)\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return [word.lower() if word not in punc_tokens.keys() else punc_tokens[word] for word in words]\n",
    "\n",
    "\n",
    "def _tokenize_word_lists(row_list, words_to_int):\n",
    "    return ' '.join([str(words_to_int[word]) for word in row_list])\n",
    "\n",
    "\n",
    "def do_split():\n",
    "    data = pd.read_csv('../data/headlines_combined.csv', header=0)\n",
    "    data['Combined'] = data.apply(lambda row: _split_remove_stopwords_tokenize_punc(row['Combined']), axis=1)\n",
    "\n",
    "    words = set()\n",
    "    for index, row in data.iterrows():\n",
    "        words.update(row[3])\n",
    "\n",
    "    words_to_int = {word: index for index, word in enumerate(words)}\n",
    "\n",
    "    data['Combined'] = data.apply(lambda row: _tokenize_word_lists(row['Combined'], words_to_int), axis=1)\n",
    "\n",
    "    data.to_csv('../data/tokenized_headlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "do_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_path = '../data/tokenized_headlines.csv'\n",
    "#string creation and batch hyperparameters\n",
    "split_frac = 0.8\n",
    "batch_size = 198\n",
    "length = 420\n",
    "subsample_thresh = 10**-5\n",
    "#network hyperparameters\n",
    "num_epochs = 100\n",
    "rnn_size = 64\n",
    "embed_dim = 150\n",
    "learning_rate = 0.0001\n",
    "dropout_keep_rate = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get Data and Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data, vocab = bldrs.get_data_and_vocab(file_path)\n",
    "train_x, val_x, test_x, train_y, val_y, test_y = bldrs.get_data_splits(split_frac, data)\n",
    "train_x, val_x, test_x = bldrs.create_lists_and_filter(train_x, val_x, test_x, subsample_thresh)\n",
    "train_batches = bldrs.get_batches_and_pad(train_x, train_y, batch_size, length)\n",
    "val_batches = bldrs.get_batches_and_pad(val_x, val_y, batch_size, length)\n",
    "test_batches = bldrs.get_batches_and_pad(test_x, test_y, batch_size, length)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "logs_path = 'logs/'\n",
    "with train_graph.as_default():\n",
    "    inputs, targets = bldrs.build_inputs_and_targets()\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    input_shape = tf.shape(inputs)\n",
    "    embedding_layer = bldrs.build_embedding_layer(inputs, vocab_size, embed_dim)\n",
    "    cell, initial_state = bldrs.build_lstm_cell(rnn_size, batch_size, keep_prob)\n",
    "    rnn, final_state = bldrs.build_rnn(cell, embedding_layer)\n",
    "    predictions = tf.contrib.layers.fully_connected(inputs=rnn[:,-1],\n",
    "                                               num_outputs=1,\n",
    "                                               activation_fn=tf.sigmoid,\n",
    "                                               weights_initializer=tf.truncated_normal_initializer(),\n",
    "                                               biases_initializer=tf.zeros_initializer(),\n",
    "                                               trainable=True)\n",
    "    cost = tf.losses.mean_squared_error(targets, predictions)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    saver = tf.train.Saver()\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), targets)\n",
    "    return_predictions = tf.cast(tf.round(predictions), tf.int32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100 Iteration: 5 Train Loss: 0.332\n",
      "Epoch: 1/100 Iteration: 10 Train Loss: 0.302\n",
      "Epoch: 1/100 Iteration: 15 Train Loss: 0.331\n",
      "Epoch: 2/100 Iteration: 20 Train Loss: 0.315\n",
      "Epoch: 3/100 Iteration: 25 Train Loss: 0.313\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 3/100 Iteration: 30 Train Loss: 0.326\n",
      "Epoch: 4/100 Iteration: 35 Train Loss: 0.307\n",
      "Epoch: 4/100 Iteration: 40 Train Loss: 0.298\n",
      "Epoch: 5/100 Iteration: 45 Train Loss: 0.310\n",
      "Epoch: 6/100 Iteration: 50 Train Loss: 0.310\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 6/100 Iteration: 55 Train Loss: 0.297\n",
      "Epoch: 7/100 Iteration: 60 Train Loss: 0.310\n",
      "Epoch: 8/100 Iteration: 65 Train Loss: 0.345\n",
      "Epoch: 8/100 Iteration: 70 Train Loss: 0.280\n",
      "Epoch: 9/100 Iteration: 75 Train Loss: 0.305\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 9/100 Iteration: 80 Train Loss: 0.262\n",
      "Epoch: 10/100 Iteration: 85 Train Loss: 0.282\n",
      "Epoch: 11/100 Iteration: 90 Train Loss: 0.286\n",
      "Epoch: 11/100 Iteration: 95 Train Loss: 0.279\n",
      "Epoch: 12/100 Iteration: 100 Train Loss: 0.302\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 13/100 Iteration: 105 Train Loss: 0.272\n",
      "Epoch: 13/100 Iteration: 110 Train Loss: 0.306\n",
      "Epoch: 14/100 Iteration: 115 Train Loss: 0.293\n",
      "Epoch: 14/100 Iteration: 120 Train Loss: 0.280\n",
      "Epoch: 15/100 Iteration: 125 Train Loss: 0.273\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 16/100 Iteration: 130 Train Loss: 0.273\n",
      "Epoch: 16/100 Iteration: 135 Train Loss: 0.259\n",
      "Epoch: 17/100 Iteration: 140 Train Loss: 0.283\n",
      "Epoch: 18/100 Iteration: 145 Train Loss: 0.298\n",
      "Epoch: 18/100 Iteration: 150 Train Loss: 0.263\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 19/100 Iteration: 155 Train Loss: 0.273\n",
      "Epoch: 19/100 Iteration: 160 Train Loss: 0.250\n",
      "Epoch: 20/100 Iteration: 165 Train Loss: 0.254\n",
      "Epoch: 21/100 Iteration: 170 Train Loss: 0.242\n",
      "Epoch: 21/100 Iteration: 175 Train Loss: 0.256\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 22/100 Iteration: 180 Train Loss: 0.277\n",
      "Epoch: 23/100 Iteration: 185 Train Loss: 0.281\n",
      "Epoch: 23/100 Iteration: 190 Train Loss: 0.249\n",
      "Epoch: 24/100 Iteration: 195 Train Loss: 0.241\n",
      "Epoch: 24/100 Iteration: 200 Train Loss: 0.241\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 25/100 Iteration: 205 Train Loss: 0.243\n",
      "Epoch: 26/100 Iteration: 210 Train Loss: 0.234\n",
      "Epoch: 26/100 Iteration: 215 Train Loss: 0.266\n",
      "Epoch: 27/100 Iteration: 220 Train Loss: 0.246\n",
      "Epoch: 28/100 Iteration: 225 Train Loss: 0.282\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 28/100 Iteration: 230 Train Loss: 0.239\n",
      "Epoch: 29/100 Iteration: 235 Train Loss: 0.254\n",
      "Epoch: 29/100 Iteration: 240 Train Loss: 0.251\n",
      "Epoch: 30/100 Iteration: 245 Train Loss: 0.232\n",
      "Epoch: 31/100 Iteration: 250 Train Loss: 0.251\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 31/100 Iteration: 255 Train Loss: 0.215\n",
      "Epoch: 32/100 Iteration: 260 Train Loss: 0.232\n",
      "Epoch: 33/100 Iteration: 265 Train Loss: 0.231\n",
      "Epoch: 33/100 Iteration: 270 Train Loss: 0.235\n",
      "Epoch: 34/100 Iteration: 275 Train Loss: 0.262\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 34/100 Iteration: 280 Train Loss: 0.237\n",
      "Epoch: 35/100 Iteration: 285 Train Loss: 0.209\n",
      "Epoch: 36/100 Iteration: 290 Train Loss: 0.209\n",
      "Epoch: 36/100 Iteration: 295 Train Loss: 0.223\n",
      "Epoch: 37/100 Iteration: 300 Train Loss: 0.225\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 38/100 Iteration: 305 Train Loss: 0.276\n",
      "Epoch: 38/100 Iteration: 310 Train Loss: 0.215\n",
      "Epoch: 39/100 Iteration: 315 Train Loss: 0.223\n",
      "Epoch: 39/100 Iteration: 320 Train Loss: 0.205\n",
      "Epoch: 40/100 Iteration: 325 Train Loss: 0.186\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 41/100 Iteration: 330 Train Loss: 0.234\n",
      "Epoch: 41/100 Iteration: 335 Train Loss: 0.225\n",
      "Epoch: 42/100 Iteration: 340 Train Loss: 0.209\n",
      "Epoch: 43/100 Iteration: 345 Train Loss: 0.233\n",
      "Epoch: 43/100 Iteration: 350 Train Loss: 0.217\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 44/100 Iteration: 355 Train Loss: 0.223\n",
      "Epoch: 44/100 Iteration: 360 Train Loss: 0.212\n",
      "Epoch: 45/100 Iteration: 365 Train Loss: 0.220\n",
      "Epoch: 46/100 Iteration: 370 Train Loss: 0.195\n",
      "Epoch: 46/100 Iteration: 375 Train Loss: 0.204\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 47/100 Iteration: 380 Train Loss: 0.212\n",
      "Epoch: 48/100 Iteration: 385 Train Loss: 0.213\n",
      "Epoch: 48/100 Iteration: 390 Train Loss: 0.186\n",
      "Epoch: 49/100 Iteration: 395 Train Loss: 0.202\n",
      "Epoch: 49/100 Iteration: 400 Train Loss: 0.213\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 50/100 Iteration: 405 Train Loss: 0.193\n",
      "Epoch: 51/100 Iteration: 410 Train Loss: 0.178\n",
      "Epoch: 51/100 Iteration: 415 Train Loss: 0.194\n",
      "Epoch: 52/100 Iteration: 420 Train Loss: 0.182\n",
      "Epoch: 53/100 Iteration: 425 Train Loss: 0.193\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 53/100 Iteration: 430 Train Loss: 0.174\n",
      "Epoch: 54/100 Iteration: 435 Train Loss: 0.192\n",
      "Epoch: 54/100 Iteration: 440 Train Loss: 0.166\n",
      "Epoch: 55/100 Iteration: 445 Train Loss: 0.191\n",
      "Epoch: 56/100 Iteration: 450 Train Loss: 0.182\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 56/100 Iteration: 455 Train Loss: 0.186\n",
      "Epoch: 57/100 Iteration: 460 Train Loss: 0.190\n",
      "Epoch: 58/100 Iteration: 465 Train Loss: 0.168\n",
      "Epoch: 58/100 Iteration: 470 Train Loss: 0.166\n",
      "Epoch: 59/100 Iteration: 475 Train Loss: 0.173\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 59/100 Iteration: 480 Train Loss: 0.156\n",
      "Epoch: 60/100 Iteration: 485 Train Loss: 0.156\n",
      "Epoch: 61/100 Iteration: 490 Train Loss: 0.178\n",
      "Epoch: 61/100 Iteration: 495 Train Loss: 0.167\n",
      "Epoch: 62/100 Iteration: 500 Train Loss: 0.169\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 63/100 Iteration: 505 Train Loss: 0.164\n",
      "Epoch: 63/100 Iteration: 510 Train Loss: 0.129\n",
      "Epoch: 64/100 Iteration: 515 Train Loss: 0.161\n",
      "Epoch: 64/100 Iteration: 520 Train Loss: 0.124\n",
      "Epoch: 65/100 Iteration: 525 Train Loss: 0.155\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 66/100 Iteration: 530 Train Loss: 0.152\n",
      "Epoch: 66/100 Iteration: 535 Train Loss: 0.144\n",
      "Epoch: 67/100 Iteration: 540 Train Loss: 0.143\n",
      "Epoch: 68/100 Iteration: 545 Train Loss: 0.152\n",
      "Epoch: 68/100 Iteration: 550 Train Loss: 0.140\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 69/100 Iteration: 555 Train Loss: 0.137\n",
      "Epoch: 69/100 Iteration: 560 Train Loss: 0.095\n",
      "Epoch: 70/100 Iteration: 565 Train Loss: 0.122\n",
      "Epoch: 71/100 Iteration: 570 Train Loss: 0.126\n",
      "Epoch: 71/100 Iteration: 575 Train Loss: 0.116\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 72/100 Iteration: 580 Train Loss: 0.127\n",
      "Epoch: 73/100 Iteration: 585 Train Loss: 0.118\n",
      "Epoch: 73/100 Iteration: 590 Train Loss: 0.139\n",
      "Epoch: 74/100 Iteration: 595 Train Loss: 0.104\n",
      "Epoch: 74/100 Iteration: 600 Train Loss: 0.108\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 75/100 Iteration: 605 Train Loss: 0.093\n",
      "Epoch: 76/100 Iteration: 610 Train Loss: 0.104\n",
      "Epoch: 76/100 Iteration: 615 Train Loss: 0.108\n",
      "Epoch: 77/100 Iteration: 620 Train Loss: 0.107\n",
      "Epoch: 78/100 Iteration: 625 Train Loss: 0.120\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 78/100 Iteration: 630 Train Loss: 0.105\n",
      "Epoch: 79/100 Iteration: 635 Train Loss: 0.109\n",
      "Epoch: 79/100 Iteration: 640 Train Loss: 0.082\n",
      "Epoch: 80/100 Iteration: 645 Train Loss: 0.086\n",
      "Epoch: 81/100 Iteration: 650 Train Loss: 0.103\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 81/100 Iteration: 655 Train Loss: 0.096\n",
      "Epoch: 82/100 Iteration: 660 Train Loss: 0.097\n",
      "Epoch: 83/100 Iteration: 665 Train Loss: 0.096\n",
      "Epoch: 83/100 Iteration: 670 Train Loss: 0.080\n",
      "Epoch: 84/100 Iteration: 675 Train Loss: 0.092\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 84/100 Iteration: 680 Train Loss: 0.075\n",
      "Epoch: 85/100 Iteration: 685 Train Loss: 0.069\n",
      "Epoch: 86/100 Iteration: 690 Train Loss: 0.087\n",
      "Epoch: 86/100 Iteration: 695 Train Loss: 0.069\n",
      "Epoch: 87/100 Iteration: 700 Train Loss: 0.085\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 88/100 Iteration: 705 Train Loss: 0.065\n",
      "Epoch: 88/100 Iteration: 710 Train Loss: 0.071\n",
      "Epoch: 89/100 Iteration: 715 Train Loss: 0.070\n",
      "Epoch: 89/100 Iteration: 720 Train Loss: 0.047\n",
      "Epoch: 90/100 Iteration: 725 Train Loss: 0.057\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 91/100 Iteration: 730 Train Loss: 0.075\n",
      "Epoch: 91/100 Iteration: 735 Train Loss: 0.047\n",
      "Epoch: 92/100 Iteration: 740 Train Loss: 0.057\n",
      "Epoch: 93/100 Iteration: 745 Train Loss: 0.052\n",
      "Epoch: 93/100 Iteration: 750 Train Loss: 0.052\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 94/100 Iteration: 755 Train Loss: 0.057\n",
      "Epoch: 94/100 Iteration: 760 Train Loss: 0.038\n",
      "Epoch: 95/100 Iteration: 765 Train Loss: 0.034\n",
      "Epoch: 96/100 Iteration: 770 Train Loss: 0.054\n",
      "Epoch: 96/100 Iteration: 775 Train Loss: 0.036\n",
      "Validation Accuracy: 0.520\n",
      "Epoch: 97/100 Iteration: 780 Train Loss: 0.041\n",
      "Epoch: 98/100 Iteration: 785 Train Loss: 0.043\n",
      "Epoch: 98/100 Iteration: 790 Train Loss: 0.041\n",
      "Epoch: 99/100 Iteration: 795 Train Loss: 0.038\n",
      "Epoch: 99/100 Iteration: 800 Train Loss: 0.037\n",
      "Validation Accuracy: 0.520\n"
     ]
    }
   ],
   "source": [
    "training_losses = []\n",
    "val_losses = [] \n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('./logs/1', sess.graph)\n",
    "    iteration = 1\n",
    "    for e in range(num_epochs):\n",
    "        state = sess.run(initial_state)\n",
    "\n",
    "        for index, batch in enumerate(train_batches):\n",
    "            feed = {inputs: batch['headlines'],\n",
    "                    targets: batch['labels'][:, None],\n",
    "                    keep_prob: dropout_keep_rate,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            training_losses.append(loss)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print('Epoch: {}/{}'.format(e, num_epochs),\n",
    "                      'Iteration: {}'.format(iteration),\n",
    "                      'Train Loss: {:.3f}'.format(loss))\n",
    "\n",
    "                # VALIDATE\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for val_index, val_batch in enumerate(val_batches):\n",
    "                    feed = {inputs: val_batch['headlines'],\n",
    "                            targets: val_batch['labels'][:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_cost, batch_acc, val_state = sess.run([cost, accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                    val_losses.append([batch_cost]*5)\n",
    "                if iteration%25==0:\n",
    "                    print(\"Validation Accuracy: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration += 1\n",
    "        saver.save(sess,\"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/sentiment.ckpt\n",
      "Test Accuracy: 0.465\n",
      "Test F score: 0.317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "test_pred = []\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for test_index, test_batch in enumerate(test_batches):\n",
    "        feed = {inputs: test_batch['headlines'],\n",
    "                targets: test_batch['labels'][:, None],\n",
    "                keep_prob: 1.0,\n",
    "                initial_state: test_state}\n",
    "        test_batch_acc, test_state, test_batch_pred = sess.run([accuracy, final_state, return_predictions], feed_dict=feed)\n",
    "        test_batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(test_batch_acc)\n",
    "        test_pred.append(test_batch_pred)\n",
    "    print(\"Test Accuracy: {:.3f}\".format(np.mean(test_acc)))\n",
    "    print(\"Test F score: {:.3f}\".format(f1_score([value[0] for sublist in test_pred for value in sublist], test_y[:batch_size],\n",
    "                                                labels=[0,1],average='macro')))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32582802, 0.31533808, 0.32229766, 0.31776801, 0.33157608, 0.36969465, 0.33539078, 0.31178212, 0.3298243, 0.30168676, 0.32244197, 0.34007615, 0.35913351, 0.31584883, 0.33050406, 0.27873898, 0.29871792, 0.29131413, 0.32853904, 0.31505021, 0.31514812, 0.33659196, 0.30192208, 0.30874711, 0.31289375, 0.28617954, 0.34263954, 0.33132657, 0.3176572, 0.3256506, 0.33359346, 0.29111952, 0.30560362, 0.28664151, 0.30696377, 0.3255063, 0.30474687, 0.29764989, 0.29767132, 0.29833421, 0.3056007, 0.27771959, 0.32021365, 0.32625267, 0.30979842, 0.30576697, 0.32174206, 0.28475836, 0.31010419, 0.309508, 0.30311418, 0.29724288, 0.29346231, 0.29508784, 0.29672691, 0.26762405, 0.29426017, 0.31177938, 0.29467252, 0.31009468, 0.27505565, 0.32041413, 0.31310827, 0.29935339, 0.34523955, 0.30087066, 0.33636108, 0.30822554, 0.29489401, 0.27950764, 0.3151769, 0.29494917, 0.27383938, 0.30040047, 0.30517492, 0.29541653, 0.27807614, 0.31087238, 0.31239766, 0.2617529, 0.28836644, 0.31070095, 0.28616849, 0.29942942, 0.28227505, 0.31211486, 0.30273667, 0.28480691, 0.31081268, 0.28585869, 0.30029887, 0.31133988, 0.28324866, 0.28906724, 0.27854794, 0.2900641, 0.27794632, 0.29077515, 0.30206934, 0.30196589, 0.26820999, 0.28992361, 0.29079106, 0.28538334, 0.27157483, 0.29985553, 0.27884001, 0.2586177, 0.29213551, 0.30627853, 0.29567355, 0.25961965, 0.30947313, 0.2705279, 0.2927877, 0.28183305, 0.28959811, 0.27428693, 0.29267073, 0.27981877, 0.27517724, 0.28159314, 0.27636221, 0.27840289, 0.27319261, 0.28665099, 0.26704013, 0.26157328, 0.29237798, 0.27261716, 0.26707345, 0.30006456, 0.26752782, 0.27192271, 0.25926647, 0.29056901, 0.29216173, 0.27959839, 0.26767066, 0.28261077, 0.25999424, 0.25424671, 0.29688215, 0.25952899, 0.29783303, 0.26938051, 0.27948639, 0.26055038, 0.28138959, 0.26256731, 0.26448569, 0.26033545, 0.30092588, 0.24569307, 0.27281877, 0.25097612, 0.24934587, 0.27217662, 0.26579916, 0.25028986, 0.30054697, 0.24906756, 0.25785258, 0.2445251, 0.25419694, 0.26531681, 0.27985698, 0.27707756, 0.27730063, 0.24176688, 0.28606844, 0.28493956, 0.23259017, 0.27365953, 0.2564078, 0.2528877, 0.27304468, 0.22780219, 0.2732431, 0.27696973, 0.25480115, 0.25671232, 0.26449102, 0.2551986, 0.28107831, 0.23797181, 0.2690672, 0.29537013, 0.29614192, 0.24907643, 0.23815785, 0.25625852, 0.26308903, 0.25166222, 0.24131243, 0.26614764, 0.22443688, 0.25451764, 0.27412844, 0.24099152, 0.28122753, 0.22216265, 0.2637637, 0.29189447, 0.2431656, 0.25373641, 0.26130611, 0.24062735, 0.25524697, 0.23416743, 0.25301635, 0.24630463, 0.23111001, 0.25901783, 0.26634967, 0.20640846, 0.2678034, 0.25118506, 0.24394661, 0.24639641, 0.24495946, 0.21802539, 0.25380278, 0.22902977, 0.28195825, 0.26012564, 0.22061974, 0.23478562, 0.22736663, 0.23865166, 0.24347056, 0.24217151, 0.23655754, 0.24935152, 0.25361446, 0.2501978, 0.25193134, 0.21438055, 0.25537252, 0.25127032, 0.24223207, 0.24142158, 0.26984796, 0.24486548, 0.23158079, 0.23154804, 0.25418106, 0.23449667, 0.25931576, 0.25132099, 0.25886372, 0.2035424, 0.22207199, 0.21192944, 0.21537773, 0.22614546, 0.25000706, 0.23841411, 0.26216704, 0.23235032, 0.22640236, 0.2265836, 0.24242322, 0.23913467, 0.23073742, 0.21231803, 0.2402374, 0.25709173, 0.22836645, 0.23541936, 0.24501078, 0.23565681, 0.25388753, 0.22995964, 0.26235056, 0.25257388, 0.2085415, 0.22554086, 0.23044689, 0.23719555, 0.22801721, 0.21785882, 0.24357869, 0.22404264, 0.20922741, 0.21129209, 0.22948569, 0.21399598, 0.23073103, 0.2085706, 0.24632265, 0.23125812, 0.20153697, 0.22309348, 0.22257842, 0.21473181, 0.24345516, 0.23116723, 0.22635363, 0.22466217, 0.2174857, 0.22991678, 0.22387476, 0.23021771, 0.27625388, 0.20579101, 0.23459835, 0.22161199, 0.20602889, 0.21465644, 0.21979193, 0.21844573, 0.25324351, 0.22787237, 0.22280687, 0.23090936, 0.2225215, 0.22971027, 0.23493688, 0.20492651, 0.25972733, 0.21811479, 0.2245288, 0.2026162, 0.18595269, 0.20090598, 0.21056929, 0.22585647, 0.24950798, 0.23407739, 0.22319624, 0.21811257, 0.20603783, 0.22777921, 0.22490764, 0.1966498, 0.257429, 0.21551679, 0.2064676, 0.20940341, 0.20180057, 0.20572932, 0.19001605, 0.20350777, 0.23308674, 0.21628924, 0.21469755, 0.22404057, 0.20968549, 0.21679306, 0.21272756, 0.19260688, 0.20273104, 0.19610646, 0.22275709, 0.23557571, 0.20564443, 0.22214679, 0.18625826, 0.21203664, 0.21820715, 0.2048125, 0.21052933, 0.22384876, 0.22004974, 0.18434301, 0.18942733, 0.21447545, 0.22593248, 0.19530015, 0.21578059, 0.19325258, 0.22753039, 0.1837216, 0.20436864, 0.20465945, 0.22278635, 0.19752191, 0.23642896, 0.21224201, 0.19739953, 0.20736346, 0.20313664, 0.17076063, 0.21318068, 0.18737213, 0.20466329, 0.21999186, 0.20017348, 0.18560658, 0.1997336, 0.18299763, 0.21806748, 0.21106015, 0.20151347, 0.20796017, 0.16975383, 0.19112329, 0.19126378, 0.2125385, 0.19971794, 0.19057925, 0.22627766, 0.19848771, 0.19321495, 0.19328293, 0.17234954, 0.18250029, 0.20457421, 0.17789248, 0.19840622, 0.20596559, 0.17123625, 0.18359892, 0.19408511, 0.19700861, 0.21395816, 0.18255195, 0.20913875, 0.1817078, 0.18373324, 0.19140811, 0.16152608, 0.1796958, 0.19331081, 0.17563489, 0.17880748, 0.17081571, 0.18089667, 0.1736057, 0.16708979, 0.1663577, 0.19814901, 0.19794609, 0.19222476, 0.19452851, 0.19581731, 0.16842133, 0.18723239, 0.16614969, 0.21904197, 0.20267798, 0.19625144, 0.17661662, 0.19067226, 0.18695042, 0.17409265, 0.16607457, 0.1684563, 0.18217923, 0.18944412, 0.18758084, 0.16814843, 0.17377256, 0.18589413, 0.15717019, 0.18358909, 0.17740156, 0.19079921, 0.19039814, 0.16390382, 0.15782374, 0.17360385, 0.15647887, 0.168184, 0.17435029, 0.17238994, 0.18471067, 0.15414557, 0.16632593, 0.20581225, 0.14279145, 0.19076076, 0.18367796, 0.17305365, 0.17871612, 0.17143913, 0.14600971, 0.16547576, 0.15612037, 0.18379341, 0.1515853, 0.15763021, 0.19804324, 0.15638764, 0.16402707, 0.16582714, 0.14486392, 0.20133097, 0.17801476, 0.16505675, 0.16618882, 0.15966417, 0.14797963, 0.16722679, 0.15381841, 0.19119757, 0.17309977, 0.15999506, 0.16901797, 0.16002962, 0.14773706, 0.16622943, 0.12557742, 0.16412583, 0.15386365, 0.15806288, 0.15804055, 0.12610054, 0.12871414, 0.1411144, 0.12060214, 0.17914884, 0.16525872, 0.16096303, 0.17908853, 0.15082462, 0.13123508, 0.15672705, 0.12414577, 0.16561736, 0.14914006, 0.16040401, 0.14469905, 0.15519693, 0.1487568, 0.14796668, 0.13650274, 0.16940559, 0.15186134, 0.14728031, 0.15109612, 0.14725186, 0.14979784, 0.14410889, 0.12262475, 0.15662512, 0.16090626, 0.14394458, 0.14252046, 0.12644815, 0.12630205, 0.154111, 0.11808795, 0.15245923, 0.14123426, 0.15722869, 0.14468306, 0.13231739, 0.14009103, 0.15066096, 0.11044295, 0.14170022, 0.12868385, 0.13713366, 0.12667453, 0.12915462, 0.1382599, 0.13847944, 0.094644971, 0.13831452, 0.13203612, 0.13993438, 0.14495434, 0.12176894, 0.13766164, 0.12181924, 0.12904662, 0.13054375, 0.12639712, 0.12661822, 0.14841466, 0.11283302, 0.12552083, 0.11625567, 0.11356458, 0.13195288, 0.1226763, 0.12965569, 0.12715524, 0.11715026, 0.13285792, 0.13551307, 0.11387075, 0.11779647, 0.13977844, 0.1262518, 0.13313431, 0.10648215, 0.13903391, 0.11226836, 0.081750974, 0.12499404, 0.12531328, 0.10442626, 0.11686767, 0.093856379, 0.12687612, 0.11528703, 0.10770641, 0.11060441, 0.11452451, 0.13513045, 0.11720102, 0.092716239, 0.11110785, 0.11479648, 0.091015235, 0.11683001, 0.10431203, 0.10878059, 0.10940118, 0.10620008, 0.11934669, 0.1078414, 0.097220495, 0.10143447, 0.11995635, 0.10246922, 0.10658972, 0.10442176, 0.098068304, 0.10110325, 0.089486197, 0.11990656, 0.11232293, 0.10283838, 0.1032159, 0.10067171, 0.10519567, 0.087623075, 0.086794749, 0.11199035, 0.095684141, 0.10912573, 0.096884847, 0.082740746, 0.092088275, 0.085434914, 0.081605062, 0.080633536, 0.089359224, 0.10191182, 0.10370094, 0.085569017, 0.10739773, 0.083424382, 0.081718452, 0.096029088, 0.10325139, 0.090126142, 0.076605499, 0.073793776, 0.10258784, 0.096032709, 0.071242258, 0.10250388, 0.10087708, 0.080235921, 0.097392783, 0.077491522, 0.093907937, 0.078724965, 0.077512167, 0.09596163, 0.089412779, 0.086634725, 0.097287834, 0.071170501, 0.079908401, 0.08003965, 0.068285942, 0.091508135, 0.10757395, 0.091607399, 0.073627576, 0.080863625, 0.080625787, 0.073813081, 0.07498996, 0.096128628, 0.08440873, 0.069565959, 0.075683556, 0.069278091, 0.080114573, 0.073743843, 0.060998149, 0.071643755, 0.087122269, 0.078854293, 0.083037741, 0.075339444, 0.077674538, 0.068948984, 0.061562043, 0.070162266, 0.074394614, 0.05743476, 0.085405491, 0.069228113, 0.073039129, 0.063082486, 0.053476699, 0.065279454, 0.084538586, 0.063886069, 0.071845375, 0.060420632, 0.070878737, 0.064562693, 0.058771253, 0.061920352, 0.0763219, 0.069713369, 0.0682761, 0.053897992, 0.06923946, 0.047129653, 0.047442332, 0.064279906, 0.066309229, 0.060165916, 0.053311482, 0.056878816, 0.060333364, 0.053881582, 0.046398655, 0.043751013, 0.075068325, 0.06430082, 0.054937959, 0.04489062, 0.074145734, 0.046787079, 0.042838443, 0.075653806, 0.063550018, 0.067385383, 0.0572404, 0.061282039, 0.054004438, 0.045443512, 0.048375323, 0.052051626, 0.060173824, 0.046375927, 0.038093932, 0.041927546, 0.051804312, 0.03862831, 0.048138961, 0.041473798, 0.052158438, 0.056704838, 0.055960748, 0.047997776, 0.053477157, 0.037432346, 0.038364686, 0.055420179, 0.058180477, 0.048624627, 0.043408908, 0.033620421, 0.045197587, 0.035018638, 0.032294106, 0.047372788, 0.05439065, 0.04741763, 0.04108616, 0.032438587, 0.037452206, 0.036442809, 0.032965619, 0.039374981, 0.037799936, 0.037972264, 0.040910773, 0.029938756, 0.038603518, 0.03486719, 0.023326594, 0.043413341, 0.048786554, 0.038394153, 0.033230491, 0.037866309, 0.040877178, 0.03493074, 0.035870392, 0.044266749, 0.043440655, 0.03775217, 0.030582948, 0.029227288, 0.036275566, 0.015872331, 0.036562875]\n"
     ]
    }
   ],
   "source": [
    "print(training_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46464646464646464, 0.53535353535353536]\n"
     ]
    }
   ],
   "source": [
    "test_points = len(test_batches)*batch_size\n",
    "bench_candidate_1 = [0]*test_points\n",
    "bench_candidate_2 = [1]*test_points\n",
    "cand_accs = [accuracy_score(bench_candidate_1, test_y[:test_points]),accuracy_score(bench_candidate_2, test_y[:test_points])]\n",
    "print(cand_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We see that the benchmark candidate containing all 1's has the better accuracy score, which is .5377."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.348684210526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "bench_f_score = f1_score(bench_candidate_2, test_y[:test_points], labels=[0,1],average='macro')\n",
    "print(bench_f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The f-score for the benchmark is 0.3497.  The warning that is printed is expected.  We are taking the mean of the f-scores for both classes.  Because there are no true positives for class 0, the f-score for that class will have 0 in the denominator, and is thus undefined.  Therefore, it is set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
